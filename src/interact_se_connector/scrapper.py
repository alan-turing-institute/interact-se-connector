import os
import re
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, MutableSet, Tuple
import pandas as pd
from bs4 import BeautifulSoup, Tag
from icecream import ic

"""
Striping out common words
https://stackoverflow.com/questions/9953619/technique-to-remove-common-wordsand-their-plural-versions-from-a-string
"""

def strip_internal_anchors(soup: Tag, class_name: str) -> Tag:

    for anchor_tags in soup.find_all("a", attrs={"class": class_name}):
        anchor_tags.decompose()

    return soup


def _do_strip_formatting(soup: Tag) -> List[str]:
    """
    Recursively search the html extracting the contents as plain text.
    This function is an intermediatory - You probably should call `strip_formatting` which returns a single `str`.

    @returns An array of strings which collectively form the plain text representation of the input html.
    """

    if not hasattr(soup, "contents"):
        # This element is text. The process here is to:
        # - Split the text into separate lines.
        # - If this results in an empty line, than add a newline char back in.
        # - For all other lines, strip out most of the white space, but ensure that there is exactly one space at the end.
        return [f"{s.strip()} " if s else "\n" for s in soup.string.splitlines()]

    result_body = []
    for child in soup.contents:
        # This element contains other elements
        # Call `_strip_formmatting` recursively on the child elements.
        result_body.extend(_do_strip_formatting(child))

    return result_body


def strip_formatting(soup: Tag) -> str:
    """ "
    Recursively search the html extracting the contents as plain text.
    The real work of this is performed by the function `_do_strip_formatting`.

    @returns An array of strings which collectively form the plain text representation of the input html.
    """
    return "".join(_do_strip_formatting(soup))


def get_keywords_from_headings(soup: Tag) -> MutableSet[str]:

    result_keywords = set()

    headers = soup.find_all(re.compile("^h\d"))
    if headers:
        for kws in headers:
            for kw in kws.stripped_strings:
                result_keywords.update(kw.split())

    return result_keywords


def reject_redirect_files(soup: Tag) -> None:
    # Check for redirect files and skip them if found
    if soup.find("meta", attrs={"http-equiv": "Refresh"}):
        raise ParseSkipFile()


# def find_suitable_scrapper(root_dir: Path):
#     candidate_scrapers = [
#         scrapper_hugo.ScrapperHugo,
#         scrapper_jupyterbook.ScrapperJupyterBook
#     ]

#     scrapper = None

#     for candidate in candidate_scrapers:
#         try:
#             scrapper = candidate(root_dir, [".html"])
#             scrapper.do_walk()
#             break
#         except ParseWrongGeneratorType:
#             pass

#     return scrapper


class ParseSkipFile(ValueError):
    """
    Used to indicate when an individual file should be skipped and not included in the index.
    """

    pass


class ParseWrongGeneratorType(ValueError):
    """
    Used to indicate when one or more files in the directory where not generated by the static site generator, suitable for the specific Scraper.
    """

    pass


class Scrapper(ABC):
    def __init__(self, root_dir: Path, allowed_extensions: List[str]):
        self.root_dir = root_dir
        self.allowed_extensions = allowed_extensions
        self.exclude_dirs = [".git", "_static"]
        self.extracted_data = None

    def do_walk(self) -> pd.DataFrame:
        df = pd.DataFrame(
            columns=["url", "id", "title", "is_public", "body", "summary", "author"]
        )

        for root, dirs, files in os.walk(self.root_dir, topdown=True):

            for exc_dir in self.exclude_dirs:
                if exc_dir in dirs:
                    dirs.remove(exc_dir)

            for file_name in files:
                # ic(file_name)
                fname = Path(root, file_name).resolve()

                if fname.suffix in self.allowed_extensions:
                    try:
                        new_row = self._scrape_file(fname)
                        df = pd.concat([df, new_row.to_frame().T], ignore_index=True)
                    except ParseSkipFile:
                        ic("skipped file", fname)
                        pass

        self.extracted_data = df
        return self.extracted_data

    def _scrape_file(self, fname: Path) -> pd.Series:
        with open(fname) as fp:
            soup = BeautifulSoup(fp, features="html5lib")

        # Perform any
        soup = self.pre_parse(soup)

        # Check that this is the correct Scraper for this file type
        # (Doing this after the `pre_parse` allows for redirect files to the skipped before they are check as the correct type)
        if not self.assert_suitable_generator(soup):
            raise ParseWrongGeneratorType(
                f"The Scraper {self.static_site_generator_name} does not seem to be the correct Scraper for the file {fname}"
            )

        try:
            return pd.Series(
                {
                    "url": self.get_url(soup),
                    "id": "{id}soup",
                    "title": self.get_title(soup),
                    "is_public": self.get_is_public(soup),
                    "body": self.get_body(soup),
                    "summary": self.get_summary(soup),
                    "author": self.get_author(soup),
                }
            )
        except AttributeError as ae:
            ic(f"error processing {fname}")
            raise ae

    @property
    def static_site_generator_name(self):
        return self.__class__.__name__

    @abstractmethod
    def pre_parse(self, soup: Tag) -> Tag:
        reject_redirect_files(soup)
        return soup

    @abstractmethod
    def assert_suitable_generator(self, soup: Tag) -> bool:
        pass

    @abstractmethod
    def get_url(self, soup: Tag) -> str:
        pass

    @abstractmethod
    def get_page_id(self, soup: Tag) -> str:
        pass

    @abstractmethod
    def get_body(self, soup: Tag) -> str:
        pass

    @abstractmethod
    def get_keywords(self, soup: Tag) -> str:
        pass

    @abstractmethod
    def get_title(self, soup: Tag) -> str:
        pass

    @abstractmethod
    def get_summary(self, soup: Tag) -> str:
        pass

    @abstractmethod
    def get_author(self, soup: Tag) -> str:
        pass

    @abstractmethod
    def get_is_public(self, soup: Tag) -> bool:
        pass
